{
    "name": "root",
    "gauges": {
        "agentHunted.Policy.Entropy.mean": {
            "value": 1.2195767164230347,
            "min": 1.1938000917434692,
            "max": 1.393399953842163,
            "count": 95
        },
        "agentHunted.Policy.Entropy.sum": {
            "value": 6849.142578125,
            "min": 2877.657958984375,
            "max": 429840.75,
            "count": 95
        },
        "agentHunted.Step.mean": {
            "value": 2709002.0,
            "min": 1759486.0,
            "max": 2709002.0,
            "count": 96
        },
        "agentHunted.Step.sum": {
            "value": 2709002.0,
            "min": 1759486.0,
            "max": 2709002.0,
            "count": 96
        },
        "agentHunted.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 42.55650329589844,
            "min": 13.237556457519531,
            "max": 44.993595123291016,
            "count": 96
        },
        "agentHunted.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 425.5650329589844,
            "min": 13.31028938293457,
            "max": 605.6001586914062,
            "count": 96
        },
        "agentHunted.Policy.ExtrinsicValueEstimate.mean": {
            "value": 42.54558181762695,
            "min": 13.299110412597656,
            "max": 45.61286926269531,
            "count": 96
        },
        "agentHunted.Policy.ExtrinsicValueEstimate.sum": {
            "value": 425.455810546875,
            "min": 14.807604789733887,
            "max": 623.3599243164062,
            "count": 96
        },
        "agentHunted.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 96
        },
        "agentHunted.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 96
        },
        "agentHunted.Losses.PolicyLoss.mean": {
            "value": 0.01806503798191746,
            "min": 0.01218492394934098,
            "max": 0.02184246665177246,
            "count": 42
        },
        "agentHunted.Losses.PolicyLoss.sum": {
            "value": 0.01806503798191746,
            "min": 0.01218492394934098,
            "max": 0.02184246665177246,
            "count": 42
        },
        "agentHunted.Losses.ValueLoss.mean": {
            "value": 10.596231174468993,
            "min": 0.7507361551125844,
            "max": 29.355473391215007,
            "count": 42
        },
        "agentHunted.Losses.ValueLoss.sum": {
            "value": 10.596231174468993,
            "min": 0.7507361551125844,
            "max": 29.355473391215007,
            "count": 42
        },
        "agentHunted.Losses.BaselineLoss.mean": {
            "value": 16.908179092407227,
            "min": 0.8656895498434702,
            "max": 40.4596357981364,
            "count": 42
        },
        "agentHunted.Losses.BaselineLoss.sum": {
            "value": 16.908179092407227,
            "min": 0.8656895498434702,
            "max": 40.4596357981364,
            "count": 42
        },
        "agentHunted.Policy.LearningRate.mean": {
            "value": 0.00027298998900333996,
            "min": 0.00027298998900333996,
            "max": 0.00028217514594161997,
            "count": 42
        },
        "agentHunted.Policy.LearningRate.sum": {
            "value": 0.00027298998900333996,
            "min": 0.00027298998900333996,
            "max": 0.00028217514594161997,
            "count": 42
        },
        "agentHunted.Policy.Epsilon.mean": {
            "value": 0.19099665999999996,
            "min": 0.19099665999999996,
            "max": 0.19405838,
            "count": 42
        },
        "agentHunted.Policy.Epsilon.sum": {
            "value": 0.19099665999999996,
            "min": 0.19099665999999996,
            "max": 0.19405838,
            "count": 42
        },
        "agentHunted.Policy.Beta.mean": {
            "value": 0.009100566334000002,
            "min": 0.009100566334000002,
            "max": 0.009406432162,
            "count": 42
        },
        "agentHunted.Policy.Beta.sum": {
            "value": 0.009100566334000002,
            "min": 0.009100566334000002,
            "max": 0.009406432162,
            "count": 42
        },
        "agentHunted.Environment.EpisodeLength.mean": {
            "value": 4999.0,
            "min": 562.0,
            "max": 4999.0,
            "count": 48
        },
        "agentHunted.Environment.EpisodeLength.sum": {
            "value": 29994.0,
            "min": 1686.0,
            "max": 288414.0,
            "count": 48
        },
        "agentHunted.Self-play.ELO.mean": {
            "value": 947.3989314743962,
            "min": 947.3989314743962,
            "max": 950.2221454998677,
            "count": 19
        },
        "agentHunted.Self-play.ELO.sum": {
            "value": 2842.1967944231887,
            "min": 2842.1967944231887,
            "max": 5700.33391989897,
            "count": 19
        },
        "agentHunted.Environment.CumulativeReward.mean": {
            "value": 777.2001419067383,
            "min": -1.6198324958483379,
            "max": 1875.0680236816406,
            "count": 57
        },
        "agentHunted.Environment.CumulativeReward.sum": {
            "value": 1554.4002838134766,
            "min": -4.859497487545013,
            "max": 6210.347539901733,
            "count": 57
        },
        "agentHunted.Policy.ExtrinsicReward.mean": {
            "value": 2827.7772064208984,
            "min": -5.859498023986816,
            "max": 3780.939453125,
            "count": 57
        },
        "agentHunted.Policy.ExtrinsicReward.sum": {
            "value": 5655.554412841797,
            "min": -17.57849407196045,
            "max": 18631.041870117188,
            "count": 57
        },
        "agentHunted.Environment.GroupCumulativeReward.mean": {
            "value": 0.0,
            "min": -1.0,
            "max": 1.8867599964141846,
            "count": 57
        },
        "agentHunted.Environment.GroupCumulativeReward.sum": {
            "value": 0.0,
            "min": -5.506319999694824,
            "max": 8.731320142745972,
            "count": 57
        },
        "agentHunter.Policy.Entropy.mean": {
            "value": 1.0695208311080933,
            "min": 1.0119693279266357,
            "max": 1.2089014053344727,
            "count": 81
        },
        "agentHunter.Policy.Entropy.sum": {
            "value": 9540.1259765625,
            "min": 8388.107421875,
            "max": 178802.328125,
            "count": 81
        },
        "agentHunter.Environment.EpisodeLength.mean": {
            "value": 4999.0,
            "min": 714.0,
            "max": 4999.0,
            "count": 57
        },
        "agentHunter.Environment.EpisodeLength.sum": {
            "value": 9998.0,
            "min": 1428.0,
            "max": 145508.0,
            "count": 57
        },
        "agentHunter.Self-play.ELO.mean": {
            "value": 1517.3912852921314,
            "min": 1516.0303282061093,
            "max": 1517.6330202635345,
            "count": 21
        },
        "agentHunter.Self-play.ELO.sum": {
            "value": 3034.782570584263,
            "min": 3032.4447553768396,
            "max": 9101.392412548514,
            "count": 21
        },
        "agentHunter.Step.mean": {
            "value": 2409538.0,
            "min": 1609760.0,
            "max": 2409538.0,
            "count": 81
        },
        "agentHunter.Step.sum": {
            "value": 2409538.0,
            "min": 1609760.0,
            "max": 2409538.0,
            "count": 81
        },
        "agentHunter.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 0.19773155450820923,
            "min": 0.040104612708091736,
            "max": 0.5558090806007385,
            "count": 81
        },
        "agentHunter.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 2.1750471591949463,
            "min": 0.40104612708091736,
            "max": 7.225518226623535,
            "count": 81
        },
        "agentHunter.Policy.ExtrinsicValueEstimate.mean": {
            "value": 0.22727595269680023,
            "min": 0.04089116305112839,
            "max": 0.5603454113006592,
            "count": 81
        },
        "agentHunter.Policy.ExtrinsicValueEstimate.sum": {
            "value": 2.500035524368286,
            "min": 0.4089116156101227,
            "max": 7.284490585327148,
            "count": 81
        },
        "agentHunter.Environment.CumulativeReward.mean": {
            "value": 1.7161604762077332,
            "min": -0.9999997913837433,
            "max": 1.9294805377721786,
            "count": 62
        },
        "agentHunter.Environment.CumulativeReward.sum": {
            "value": 6.864641904830933,
            "min": -2.869599401950836,
            "max": 12.485963326878846,
            "count": 62
        },
        "agentHunter.Policy.ExtrinsicReward.mean": {
            "value": 7.990759998559952,
            "min": -1.9999995827674866,
            "max": 11.36747932434082,
            "count": 62
        },
        "agentHunter.Policy.ExtrinsicReward.sum": {
            "value": 31.963039994239807,
            "min": -5.739198803901672,
            "max": 62.170481860637665,
            "count": 62
        },
        "agentHunter.Environment.GroupCumulativeReward.mean": {
            "value": 4.558440014719963,
            "min": 0.0,
            "max": 6.567080020904541,
            "count": 62
        },
        "agentHunter.Environment.GroupCumulativeReward.sum": {
            "value": 18.233760058879852,
            "min": 0.0,
            "max": 36.1985604763031,
            "count": 62
        },
        "agentHunter.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 81
        },
        "agentHunter.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 81
        },
        "agentHunter.Losses.PolicyLoss.mean": {
            "value": 0.015111417106042305,
            "min": 0.012635726140191157,
            "max": 0.021904240486522515,
            "count": 36
        },
        "agentHunter.Losses.PolicyLoss.sum": {
            "value": 0.015111417106042305,
            "min": 0.012635726140191157,
            "max": 0.021904240486522515,
            "count": 36
        },
        "agentHunter.Losses.ValueLoss.mean": {
            "value": 0.04251530493299167,
            "min": 0.0022629226189261925,
            "max": 0.04251530493299167,
            "count": 36
        },
        "agentHunter.Losses.ValueLoss.sum": {
            "value": 0.04251530493299167,
            "min": 0.0022629226189261925,
            "max": 0.04251530493299167,
            "count": 36
        },
        "agentHunter.Losses.BaselineLoss.mean": {
            "value": 0.04887497425079346,
            "min": 0.0022761912744802735,
            "max": 0.04887497425079346,
            "count": 36
        },
        "agentHunter.Losses.BaselineLoss.sum": {
            "value": 0.04887497425079346,
            "min": 0.0022761912744802735,
            "max": 0.04887497425079346,
            "count": 36
        },
        "agentHunter.Policy.LearningRate.mean": {
            "value": 0.00027606140797953327,
            "min": 0.00027606140797953327,
            "max": 0.00028372996542334677,
            "count": 36
        },
        "agentHunter.Policy.LearningRate.sum": {
            "value": 0.00027606140797953327,
            "min": 0.00027606140797953327,
            "max": 0.00028372996542334677,
            "count": 36
        },
        "agentHunter.Policy.Epsilon.mean": {
            "value": 0.1920204666666666,
            "min": 0.1920204666666666,
            "max": 0.19457665333333343,
            "count": 36
        },
        "agentHunter.Policy.Epsilon.sum": {
            "value": 0.1920204666666666,
            "min": 0.1920204666666666,
            "max": 0.19457665333333343,
            "count": 36
        },
        "agentHunter.Policy.Beta.mean": {
            "value": 0.009202844620000001,
            "min": 0.009202844620000001,
            "max": 0.009458207668,
            "count": 36
        },
        "agentHunter.Policy.Beta.sum": {
            "value": 0.009202844620000001,
            "min": 0.009202844620000001,
            "max": 0.009458207668,
            "count": 36
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1726200165",
        "python_version": "3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Users\\nacho\\anaconda3\\envs\\cursoml5\\Scripts\\mlagents-learn ./trainer_config_Multi_1.yaml --run-id MultiAgent_4 --resume",
        "mlagents_version": "1.1.0.dev0",
        "mlagents_envs_version": "1.1.0.dev0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.2+cu121",
        "numpy_version": "1.22.4",
        "end_time_seconds": "1726203921"
    },
    "total": 3755.6663299000356,
    "count": 1,
    "self": 0.141883899923414,
    "children": {
        "run_training.setup": {
            "total": 0.2605294000823051,
            "count": 1,
            "self": 0.2605294000823051
        },
        "TrainerController.start_learning": {
            "total": 3755.26391660003,
            "count": 1,
            "self": 3.771767033962533,
            "children": {
                "TrainerController._reset_env": {
                    "total": 58.52067650016397,
                    "count": 10,
                    "self": 58.52067650016397
                },
                "TrainerController.advance": {
                    "total": 3691.801084965933,
                    "count": 184189,
                    "self": 4.596963602816686,
                    "children": {
                        "env_step": {
                            "total": 2756.243310046615,
                            "count": 184189,
                            "self": 1826.2660537254997,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 927.5734717322048,
                                    "count": 184189,
                                    "self": 22.190043318783864,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 905.3834284134209,
                                            "count": 368094,
                                            "self": 905.3834284134209
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.4037845889106393,
                                    "count": 184188,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 3633.0095617873594,
                                            "count": 184188,
                                            "is_parallel": true,
                                            "self": 2213.7137532019988,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.010003898991271853,
                                                    "count": 20,
                                                    "is_parallel": true,
                                                    "self": 0.003142197849228978,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.006861701142042875,
                                                            "count": 80,
                                                            "is_parallel": true,
                                                            "self": 0.006861701142042875
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1419.2858046863694,
                                                    "count": 184188,
                                                    "is_parallel": true,
                                                    "self": 47.11423656018451,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 47.47572855418548,
                                                            "count": 184188,
                                                            "is_parallel": true,
                                                            "self": 47.47572855418548
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1145.7984826017637,
                                                            "count": 184188,
                                                            "is_parallel": true,
                                                            "self": 1145.7984826017637
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 178.8973569702357,
                                                            "count": 368376,
                                                            "is_parallel": true,
                                                            "self": 56.02560180402361,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 122.8717551662121,
                                                                    "count": 1473504,
                                                                    "is_parallel": true,
                                                                    "self": 122.8717551662121
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 930.9608113165013,
                            "count": 368376,
                            "self": 21.192050945246592,
                            "children": {
                                "process_trajectory": {
                                    "total": 219.83620207104832,
                                    "count": 368376,
                                    "self": 219.11051217117347,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.7256898998748511,
                                            "count": 3,
                                            "self": 0.7256898998748511
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 689.9325583002064,
                                    "count": 78,
                                    "self": 276.1321317066904,
                                    "children": {
                                        "TorchPOCAOptimizer.update": {
                                            "total": 413.800426593516,
                                            "count": 2415,
                                            "self": 413.800426593516
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 1.170388099970296,
                    "count": 1,
                    "self": 0.5370074999518692,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.6333806000184268,
                            "count": 2,
                            "self": 0.6333806000184268
                        }
                    }
                }
            }
        }
    }
}