{
    "name": "root",
    "gauges": {
        "agentHunter.Policy.Entropy.mean": {
            "value": 0.4805987477302551,
            "min": 0.43572741746902466,
            "max": 0.6591641902923584,
            "count": 139
        },
        "agentHunter.Policy.Entropy.sum": {
            "value": 4965.54638671875,
            "min": 3774.25146484375,
            "max": 87314.515625,
            "count": 139
        },
        "agentHunter.Environment.EpisodeLength.mean": {
            "value": 195.23076923076923,
            "min": 139.11111111111111,
            "max": 278.6666666666667,
            "count": 139
        },
        "agentHunter.Environment.EpisodeLength.sum": {
            "value": 10152.0,
            "min": 8942.0,
            "max": 143300.0,
            "count": 139
        },
        "agentHunter.Self-play.ELO.mean": {
            "value": 1308.561195645292,
            "min": 1198.5074349501012,
            "max": 1349.704700827047,
            "count": 139
        },
        "agentHunter.Self-play.ELO.sum": {
            "value": 57576.69260839284,
            "min": 40669.80821108067,
            "max": 89028.81366124522,
            "count": 139
        },
        "agentHunter.Step.mean": {
            "value": 1389664.0,
            "min": 9998.0,
            "max": 1389664.0,
            "count": 139
        },
        "agentHunter.Step.sum": {
            "value": 1389664.0,
            "min": 9998.0,
            "max": 1389664.0,
            "count": 139
        },
        "agentHunter.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 2.1060843467712402,
            "min": 1.3073023557662964,
            "max": 2.581911087036133,
            "count": 139
        },
        "agentHunter.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 109.5163803100586,
            "min": 52.73033905029297,
            "max": 181.75099182128906,
            "count": 139
        },
        "agentHunter.Policy.ExtrinsicValueEstimate.mean": {
            "value": 2.1166584491729736,
            "min": 1.3338992595672607,
            "max": 2.60506534576416,
            "count": 139
        },
        "agentHunter.Policy.ExtrinsicValueEstimate.sum": {
            "value": 110.06623840332031,
            "min": 52.70070266723633,
            "max": 183.00186157226562,
            "count": 139
        },
        "agentHunter.Environment.CumulativeReward.mean": {
            "value": 1.3467413518600762,
            "min": 0.7994700425407952,
            "max": 1.7713782201811183,
            "count": 139
        },
        "agentHunter.Environment.CumulativeReward.sum": {
            "value": 70.03055029672396,
            "min": 35.97615191433579,
            "max": 126.00958988722414,
            "count": 139
        },
        "agentHunter.Policy.ExtrinsicReward.mean": {
            "value": 5.420808782944312,
            "min": 3.4699710263146293,
            "max": 7.102481541086416,
            "count": 139
        },
        "agentHunter.Policy.ExtrinsicReward.sum": {
            "value": 281.88205671310425,
            "min": 156.14869618415833,
            "max": 478.1434106826782,
            "count": 139
        },
        "agentHunter.Environment.GroupCumulativeReward.mean": {
            "value": 2.566863639788194,
            "min": 1.5484977651525427,
            "max": 3.710199012988951,
            "count": 139
        },
        "agentHunter.Environment.GroupCumulativeReward.sum": {
            "value": 112.94200015068054,
            "min": 65.888880610466,
            "max": 206.87863862514496,
            "count": 139
        },
        "agentHunter.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 139
        },
        "agentHunter.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 139
        },
        "agentHunter.Losses.PolicyLoss.mean": {
            "value": 0.012063001251469056,
            "min": 0.011997179609412949,
            "max": 0.02342020363236467,
            "count": 67
        },
        "agentHunter.Losses.PolicyLoss.sum": {
            "value": 0.012063001251469056,
            "min": 0.011997179609412949,
            "max": 0.02342020363236467,
            "count": 67
        },
        "agentHunter.Losses.ValueLoss.mean": {
            "value": 0.6892293492952982,
            "min": 0.515658974647522,
            "max": 0.7876706918080648,
            "count": 67
        },
        "agentHunter.Losses.ValueLoss.sum": {
            "value": 0.6892293492952982,
            "min": 0.515658974647522,
            "max": 0.7876706918080648,
            "count": 67
        },
        "agentHunter.Losses.BaselineLoss.mean": {
            "value": 0.7042855540911357,
            "min": 0.5463143636782964,
            "max": 0.8241774002710979,
            "count": 67
        },
        "agentHunter.Losses.BaselineLoss.sum": {
            "value": 0.7042855540911357,
            "min": 0.5463143636782964,
            "max": 0.8241774002710979,
            "count": 67
        },
        "agentHunter.Policy.LearningRate.mean": {
            "value": 0.0002861266446244533,
            "min": 0.0002861266446244533,
            "max": 0.00029979316006894667,
            "count": 67
        },
        "agentHunter.Policy.LearningRate.sum": {
            "value": 0.0002861266446244533,
            "min": 0.0002861266446244533,
            "max": 0.00029979316006894667,
            "count": 67
        },
        "agentHunter.Policy.Epsilon.mean": {
            "value": 0.19537554666666673,
            "min": 0.19537554666666673,
            "max": 0.19993105333333336,
            "count": 67
        },
        "agentHunter.Policy.Epsilon.sum": {
            "value": 0.19537554666666673,
            "min": 0.19537554666666673,
            "max": 0.19993105333333336,
            "count": 67
        },
        "agentHunter.Policy.Beta.mean": {
            "value": 0.009538017111999997,
            "min": 0.009538017111999997,
            "max": 0.009993112227999998,
            "count": 67
        },
        "agentHunter.Policy.Beta.sum": {
            "value": 0.009538017111999997,
            "min": 0.009538017111999997,
            "max": 0.009993112227999998,
            "count": 67
        },
        "agentHunted.Policy.Entropy.mean": {
            "value": 0.787353515625,
            "min": 0.7602248787879944,
            "max": 0.9230750799179077,
            "count": 120
        },
        "agentHunted.Policy.Entropy.sum": {
            "value": 7582.21435546875,
            "min": 5373.4111328125,
            "max": 279159.09375,
            "count": 120
        },
        "agentHunted.Environment.EpisodeLength.mean": {
            "value": 203.66666666666666,
            "min": 140.5909090909091,
            "max": 347.1,
            "count": 120
        },
        "agentHunted.Environment.EpisodeLength.sum": {
            "value": 9165.0,
            "min": 6819.0,
            "max": 308646.0,
            "count": 120
        },
        "agentHunted.Self-play.ELO.mean": {
            "value": 1170.0254251536066,
            "min": 1083.8666095919368,
            "max": 1185.9491594454921,
            "count": 120
        },
        "agentHunted.Self-play.ELO.sum": {
            "value": 45630.99158099066,
            "min": 26651.01126670948,
            "max": 76203.39965778025,
            "count": 120
        },
        "agentHunted.Step.mean": {
            "value": 1199810.0,
            "min": 9902.0,
            "max": 1199810.0,
            "count": 120
        },
        "agentHunted.Step.sum": {
            "value": 1199810.0,
            "min": 9902.0,
            "max": 1199810.0,
            "count": 120
        },
        "agentHunted.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 1.3446457386016846,
            "min": 0.6372343897819519,
            "max": 2.6148462295532227,
            "count": 120
        },
        "agentHunted.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 56.475120544433594,
            "min": 17.84256362915039,
            "max": 190.88377380371094,
            "count": 120
        },
        "agentHunted.Policy.ExtrinsicValueEstimate.mean": {
            "value": 1.4011611938476562,
            "min": 0.6345054507255554,
            "max": 2.6003966331481934,
            "count": 120
        },
        "agentHunted.Policy.ExtrinsicValueEstimate.sum": {
            "value": 58.84877014160156,
            "min": 17.76615333557129,
            "max": 189.82894897460938,
            "count": 120
        },
        "agentHunted.Environment.CumulativeReward.mean": {
            "value": 0.4295550757752998,
            "min": -0.14287235218517738,
            "max": 1.100428755171597,
            "count": 120
        },
        "agentHunted.Environment.CumulativeReward.sum": {
            "value": 18.04131318256259,
            "min": -8.286596426740289,
            "max": 55.02143775857985,
            "count": 120
        },
        "agentHunted.Policy.ExtrinsicReward.mean": {
            "value": 4.143388853186653,
            "min": 0.5192026136250332,
            "max": 6.686011306941509,
            "count": 120
        },
        "agentHunted.Policy.ExtrinsicReward.sum": {
            "value": 174.02233183383942,
            "min": 30.024398028850555,
            "max": 355.67635238170624,
            "count": 120
        },
        "agentHunted.Environment.GroupCumulativeReward.mean": {
            "value": 2.6624789912449685,
            "min": 0.1861103423710527,
            "max": 3.593418724834919,
            "count": 120
        },
        "agentHunted.Environment.GroupCumulativeReward.sum": {
            "value": 101.17420166730881,
            "min": 5.397199928760529,
            "max": 167.71120089292526,
            "count": 120
        },
        "agentHunted.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 120
        },
        "agentHunted.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 120
        },
        "agentHunted.Losses.PolicyLoss.mean": {
            "value": 0.017935258972768982,
            "min": 0.011057278707933922,
            "max": 0.024350287714817873,
            "count": 57
        },
        "agentHunted.Losses.PolicyLoss.sum": {
            "value": 0.017935258972768982,
            "min": 0.011057278707933922,
            "max": 0.024350287714817873,
            "count": 57
        },
        "agentHunted.Losses.ValueLoss.mean": {
            "value": 0.6612468600273133,
            "min": 0.4957092583179474,
            "max": 0.7635304212570191,
            "count": 57
        },
        "agentHunted.Losses.ValueLoss.sum": {
            "value": 0.6612468600273133,
            "min": 0.4957092583179474,
            "max": 0.7635304212570191,
            "count": 57
        },
        "agentHunted.Losses.BaselineLoss.mean": {
            "value": 0.7066455026467641,
            "min": 0.5288684735695521,
            "max": 0.8663248976071676,
            "count": 57
        },
        "agentHunted.Losses.BaselineLoss.sum": {
            "value": 0.7066455026467641,
            "min": 0.5288684735695521,
            "max": 0.8663248976071676,
            "count": 57
        },
        "agentHunted.Policy.LearningRate.mean": {
            "value": 0.00028810776396407994,
            "min": 0.00028810776396407994,
            "max": 0.00029979315006894996,
            "count": 57
        },
        "agentHunted.Policy.LearningRate.sum": {
            "value": 0.00028810776396407994,
            "min": 0.00028810776396407994,
            "max": 0.00029979315006894996,
            "count": 57
        },
        "agentHunted.Policy.Epsilon.mean": {
            "value": 0.19603591999999997,
            "min": 0.19603591999999997,
            "max": 0.19993105,
            "count": 57
        },
        "agentHunted.Policy.Epsilon.sum": {
            "value": 0.19603591999999997,
            "min": 0.19603591999999997,
            "max": 0.19993105,
            "count": 57
        },
        "agentHunted.Policy.Beta.mean": {
            "value": 0.009603988407999997,
            "min": 0.009603988407999997,
            "max": 0.009993111895000002,
            "count": 57
        },
        "agentHunted.Policy.Beta.sum": {
            "value": 0.009603988407999997,
            "min": 0.009603988407999997,
            "max": 0.009993111895000002,
            "count": 57
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1726612210",
        "python_version": "3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Users\\nacho\\anaconda3\\envs\\cursoml5\\Scripts\\mlagents-learn ./trainer_config_Multi_1.yaml --initialize-from=MultiAgent_6 --run-id MultiAgent_6_2",
        "mlagents_version": "1.1.0.dev0",
        "mlagents_envs_version": "1.1.0.dev0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.2+cu121",
        "numpy_version": "1.22.4",
        "end_time_seconds": "1726617149"
    },
    "total": 4938.675006299978,
    "count": 1,
    "self": 0.3774287999840453,
    "children": {
        "run_training.setup": {
            "total": 0.2903766999952495,
            "count": 1,
            "self": 0.2903766999952495
        },
        "TrainerController.start_learning": {
            "total": 4938.007200799999,
            "count": 1,
            "self": 4.410530695517082,
            "children": {
                "TrainerController._reset_env": {
                    "total": 20.925047500029905,
                    "count": 13,
                    "self": 20.925047500029905
                },
                "TrainerController.advance": {
                    "total": 4911.867197804444,
                    "count": 218707,
                    "self": 5.370532195724081,
                    "children": {
                        "env_step": {
                            "total": 3227.765988599567,
                            "count": 218707,
                            "self": 2214.436080102838,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 1010.5412086982687,
                                    "count": 218708,
                                    "self": 25.362085185188334,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 985.1791235130804,
                                            "count": 428176,
                                            "self": 985.1791235130804
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.788699798460584,
                                    "count": 218706,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 4809.843682907376,
                                            "count": 218706,
                                            "is_parallel": true,
                                            "self": 3148.8252834108716,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.08889310003723949,
                                                    "count": 28,
                                                    "is_parallel": true,
                                                    "self": 0.006951699935598299,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.08194140010164119,
                                                            "count": 112,
                                                            "is_parallel": true,
                                                            "self": 0.08194140010164119
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1660.9295063964673,
                                                    "count": 218706,
                                                    "is_parallel": true,
                                                    "self": 61.799188399949344,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 67.59257300142781,
                                                            "count": 218706,
                                                            "is_parallel": true,
                                                            "self": 67.59257300142781
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1308.9604088000488,
                                                            "count": 218706,
                                                            "is_parallel": true,
                                                            "self": 1308.9604088000488
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 222.57733619504143,
                                                            "count": 437412,
                                                            "is_parallel": true,
                                                            "self": 66.35286658717087,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 156.22446960787056,
                                                                    "count": 1749648,
                                                                    "is_parallel": true,
                                                                    "self": 156.22446960787056
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 1678.7306770091527,
                            "count": 437412,
                            "self": 27.523573223268613,
                            "children": {
                                "process_trajectory": {
                                    "total": 376.6149182860099,
                                    "count": 437412,
                                    "self": 375.005956486013,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 1.608961799996905,
                                            "count": 4,
                                            "self": 1.608961799996905
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 1274.5921854998742,
                                    "count": 124,
                                    "self": 403.5571207985922,
                                    "children": {
                                        "TorchPOCAOptimizer.update": {
                                            "total": 871.035064701282,
                                            "count": 3720,
                                            "self": 871.035064701282
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.8044248000078369,
                    "count": 1,
                    "self": 0.18868989998009056,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.6157349000277463,
                            "count": 2,
                            "self": 0.6157349000277463
                        }
                    }
                }
            }
        }
    }
}