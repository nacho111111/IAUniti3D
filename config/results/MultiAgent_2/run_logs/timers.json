{
    "name": "root",
    "gauges": {
        "agentHunted.Policy.Entropy.mean": {
            "value": 1.4700788259506226,
            "min": 1.4547715187072754,
            "max": 1.6091548204421997,
            "count": 140
        },
        "agentHunted.Policy.Entropy.sum": {
            "value": 15224.13671875,
            "min": 11393.93359375,
            "max": 495466.5625,
            "count": 140
        },
        "agentHunted.Step.mean": {
            "value": 1399485.0,
            "min": 9000.0,
            "max": 1399485.0,
            "count": 140
        },
        "agentHunted.Step.sum": {
            "value": 1399485.0,
            "min": 9000.0,
            "max": 1399485.0,
            "count": 140
        },
        "agentHunted.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": -0.5290766954421997,
            "min": -0.5674604177474976,
            "max": 0.06493902951478958,
            "count": 140
        },
        "agentHunted.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": -7.936150550842285,
            "min": -7.936150550842285,
            "max": 0.6493638753890991,
            "count": 140
        },
        "agentHunted.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.5297642350196838,
            "min": -0.5691297650337219,
            "max": 0.06493208557367325,
            "count": 140
        },
        "agentHunted.Policy.ExtrinsicValueEstimate.sum": {
            "value": -7.946463584899902,
            "min": -7.946463584899902,
            "max": 0.6492489576339722,
            "count": 140
        },
        "agentHunted.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 140
        },
        "agentHunted.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 140
        },
        "agentHunted.Environment.EpisodeLength.mean": {
            "value": 1483.0,
            "min": 890.0,
            "max": 4999.0,
            "count": 115
        },
        "agentHunted.Environment.EpisodeLength.sum": {
            "value": 13347.0,
            "min": 2670.0,
            "max": 313671.0,
            "count": 115
        },
        "agentHunted.Environment.CumulativeReward.mean": {
            "value": -2.5352753903716803,
            "min": -7.999880611896515,
            "max": 2.086162567138672e-07,
            "count": 123
        },
        "agentHunted.Environment.CumulativeReward.sum": {
            "value": -20.282203122973442,
            "min": -33.06856577843428,
            "max": 2.086162567138672e-07,
            "count": 123
        },
        "agentHunted.Policy.ExtrinsicReward.mean": {
            "value": -10.088259071111679,
            "min": -14.869925399621328,
            "max": -1.3883200883865356,
            "count": 123
        },
        "agentHunted.Policy.ExtrinsicReward.sum": {
            "value": -80.70607256889343,
            "min": -108.20571291446686,
            "max": -4.164960265159607,
            "count": 123
        },
        "agentHunted.Environment.GroupCumulativeReward.mean": {
            "value": -0.5449299812316895,
            "min": -1.0,
            "max": 1.7817280173301697,
            "count": 123
        },
        "agentHunted.Environment.GroupCumulativeReward.sum": {
            "value": -4.359439849853516,
            "min": -9.0,
            "max": 8.908640086650848,
            "count": 123
        },
        "agentHunted.Losses.PolicyLoss.mean": {
            "value": 0.01295941014153262,
            "min": 0.011307506550413867,
            "max": 0.022440199585010607,
            "count": 65
        },
        "agentHunted.Losses.PolicyLoss.sum": {
            "value": 0.01295941014153262,
            "min": 0.011307506550413867,
            "max": 0.022440199585010607,
            "count": 65
        },
        "agentHunted.Losses.ValueLoss.mean": {
            "value": 0.10235468670725822,
            "min": 0.003225979950123777,
            "max": 0.12103968386848768,
            "count": 65
        },
        "agentHunted.Losses.ValueLoss.sum": {
            "value": 0.10235468670725822,
            "min": 0.003225979950123777,
            "max": 0.12103968386848768,
            "count": 65
        },
        "agentHunted.Losses.BaselineLoss.mean": {
            "value": 0.1034245585401853,
            "min": 0.0035083472806339464,
            "max": 0.1217348501086235,
            "count": 65
        },
        "agentHunted.Losses.BaselineLoss.sum": {
            "value": 0.1034245585401853,
            "min": 0.0035083472806339464,
            "max": 0.1217348501086235,
            "count": 65
        },
        "agentHunted.Policy.LearningRate.mean": {
            "value": 0.00028609929463357004,
            "min": 0.00028609929463357004,
            "max": 0.00029979000007,
            "count": 65
        },
        "agentHunted.Policy.LearningRate.sum": {
            "value": 0.00028609929463357004,
            "min": 0.00028609929463357004,
            "max": 0.00029979000007,
            "count": 65
        },
        "agentHunted.Policy.Epsilon.mean": {
            "value": 0.19536643,
            "min": 0.19536643,
            "max": 0.19993000000000005,
            "count": 65
        },
        "agentHunted.Policy.Epsilon.sum": {
            "value": 0.19536643,
            "min": 0.19536643,
            "max": 0.19993000000000005,
            "count": 65
        },
        "agentHunted.Policy.Beta.mean": {
            "value": 0.009537106357,
            "min": 0.009537106357,
            "max": 0.009993007,
            "count": 65
        },
        "agentHunted.Policy.Beta.sum": {
            "value": 0.009537106357,
            "min": 0.009537106357,
            "max": 0.009993007,
            "count": 65
        },
        "agentHunter.Policy.Entropy.mean": {
            "value": 0.7269726395606995,
            "min": 0.6244027614593506,
            "max": 1.609209656715393,
            "count": 131
        },
        "agentHunter.Policy.Entropy.sum": {
            "value": 7108.33837890625,
            "min": 6778.5166015625,
            "max": 231724.984375,
            "count": 131
        },
        "agentHunter.Environment.EpisodeLength.mean": {
            "value": 1471.25,
            "min": 993.5,
            "max": 4999.0,
            "count": 131
        },
        "agentHunter.Environment.EpisodeLength.sum": {
            "value": 11770.0,
            "min": 6344.0,
            "max": 139466.0,
            "count": 131
        },
        "agentHunter.Step.mean": {
            "value": 1309827.0,
            "min": 9000.0,
            "max": 1309827.0,
            "count": 131
        },
        "agentHunter.Step.sum": {
            "value": 1309827.0,
            "min": 9000.0,
            "max": 1309827.0,
            "count": 131
        },
        "agentHunter.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 0.6766207814216614,
            "min": 0.011880405247211456,
            "max": 0.7835763096809387,
            "count": 131
        },
        "agentHunter.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 9.47269058227539,
            "min": 0.11880405247211456,
            "max": 12.53722095489502,
            "count": 131
        },
        "agentHunter.Policy.ExtrinsicValueEstimate.mean": {
            "value": 0.6714282631874084,
            "min": 0.010611551813781261,
            "max": 0.781894326210022,
            "count": 131
        },
        "agentHunter.Policy.ExtrinsicValueEstimate.sum": {
            "value": 9.399995803833008,
            "min": 0.10611552000045776,
            "max": 12.524356842041016,
            "count": 131
        },
        "agentHunter.Environment.CumulativeReward.mean": {
            "value": 2.4177034742065837,
            "min": -0.4999595731496811,
            "max": 2.9666674733161926,
            "count": 130
        },
        "agentHunter.Environment.CumulativeReward.sum": {
            "value": 16.923924319446087,
            "min": -0.9999191462993622,
            "max": 26.885365813970566,
            "count": 130
        },
        "agentHunter.Policy.ExtrinsicReward.mean": {
            "value": 10.752948352268763,
            "min": -0.19379973411560059,
            "max": 11.495736122131348,
            "count": 130
        },
        "agentHunter.Policy.ExtrinsicReward.sum": {
            "value": 75.27063846588135,
            "min": -0.38759946823120117,
            "max": 114.95736122131348,
            "count": 130
        },
        "agentHunter.Environment.GroupCumulativeReward.mean": {
            "value": 5.774685757500785,
            "min": 0.0,
            "max": 6.1421098709106445,
            "count": 130
        },
        "agentHunter.Environment.GroupCumulativeReward.sum": {
            "value": 40.42280030250549,
            "min": 0.0,
            "max": 61.18664073944092,
            "count": 130
        },
        "agentHunter.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 131
        },
        "agentHunter.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 131
        },
        "agentHunter.Losses.PolicyLoss.mean": {
            "value": 0.014770932076498867,
            "min": 0.011865255034839114,
            "max": 0.02693916982971132,
            "count": 61
        },
        "agentHunter.Losses.PolicyLoss.sum": {
            "value": 0.014770932076498867,
            "min": 0.011865255034839114,
            "max": 0.02693916982971132,
            "count": 61
        },
        "agentHunter.Losses.ValueLoss.mean": {
            "value": 0.06326998571554819,
            "min": 0.002830118255224079,
            "max": 0.06625530210634073,
            "count": 61
        },
        "agentHunter.Losses.ValueLoss.sum": {
            "value": 0.06326998571554819,
            "min": 0.002830118255224079,
            "max": 0.06625530210634073,
            "count": 61
        },
        "agentHunter.Losses.BaselineLoss.mean": {
            "value": 0.06354012613495191,
            "min": 0.0028571865676591795,
            "max": 0.0674057478706042,
            "count": 61
        },
        "agentHunter.Losses.BaselineLoss.sum": {
            "value": 0.06354012613495191,
            "min": 0.0028571865676591795,
            "max": 0.0674057478706042,
            "count": 61
        },
        "agentHunter.Policy.LearningRate.mean": {
            "value": 0.0002869056843647734,
            "min": 0.0002869056843647734,
            "max": 0.0002997800000733333,
            "count": 61
        },
        "agentHunter.Policy.LearningRate.sum": {
            "value": 0.0002869056843647734,
            "min": 0.0002869056843647734,
            "max": 0.0002997800000733333,
            "count": 61
        },
        "agentHunter.Policy.Epsilon.mean": {
            "value": 0.19563522666666672,
            "min": 0.19563522666666672,
            "max": 0.1999266666666666,
            "count": 61
        },
        "agentHunter.Policy.Epsilon.sum": {
            "value": 0.19563522666666672,
            "min": 0.19563522666666672,
            "max": 0.1999266666666666,
            "count": 61
        },
        "agentHunter.Policy.Beta.mean": {
            "value": 0.009563959143999998,
            "min": 0.009563959143999998,
            "max": 0.009992673999999998,
            "count": 61
        },
        "agentHunter.Policy.Beta.sum": {
            "value": 0.009563959143999998,
            "min": 0.009563959143999998,
            "max": 0.009992673999999998,
            "count": 61
        },
        "agentHunter.Self-play.ELO.mean": {
            "value": 1368.7077195925785,
            "min": 1201.2474841477301,
            "max": 1368.7077195925785,
            "count": 84
        },
        "agentHunter.Self-play.ELO.sum": {
            "value": 10949.661756740628,
            "min": 2405.4741564436995,
            "max": 13466.125339848048,
            "count": 84
        },
        "agentHunted.Self-play.ELO.mean": {
            "value": 1061.3892448720678,
            "min": 1061.3892448720678,
            "max": 1196.0490634916193,
            "count": 70
        },
        "agentHunted.Self-play.ELO.sum": {
            "value": 9552.50320384861,
            "min": 3195.3019373248662,
            "max": 10084.999001101483,
            "count": 70
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1724016494",
        "python_version": "3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Users\\nacho\\anaconda3\\envs\\cursoml5\\Scripts\\mlagents-learn ./trainer_config_Multi_1.yaml --run-id MultiAgent_2",
        "mlagents_version": "1.1.0.dev0",
        "mlagents_envs_version": "1.1.0.dev0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.2+cu121",
        "numpy_version": "1.22.4",
        "end_time_seconds": "1724028674"
    },
    "total": 12180.950012999994,
    "count": 1,
    "self": 0.10535120000713505,
    "children": {
        "run_training.setup": {
            "total": 0.26952289999462664,
            "count": 1,
            "self": 0.26952289999462664
        },
        "TrainerController.start_learning": {
            "total": 12180.575138899992,
            "count": 1,
            "self": 22.961456597899087,
            "children": {
                "TrainerController._reset_env": {
                    "total": 29.770955400017556,
                    "count": 14,
                    "self": 29.770955400017556
                },
                "TrainerController.advance": {
                    "total": 12127.363935702073,
                    "count": 1127001,
                    "self": 26.10081661643926,
                    "children": {
                        "env_step": {
                            "total": 10681.72442449868,
                            "count": 1127001,
                            "self": 5586.087008091767,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 5082.091331809905,
                                    "count": 1127001,
                                    "self": 119.35829732773709,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 4962.733034482168,
                                            "count": 2253346,
                                            "self": 4962.733034482168
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 13.546084597008303,
                                    "count": 1127000,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 12053.970089399343,
                                            "count": 1127000,
                                            "is_parallel": true,
                                            "self": 7878.70673838185,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.03550769996945746,
                                                    "count": 28,
                                                    "is_parallel": true,
                                                    "self": 0.004449699597898871,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.03105800037155859,
                                                            "count": 112,
                                                            "is_parallel": true,
                                                            "self": 0.03105800037155859
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 4175.227843317523,
                                                    "count": 1127000,
                                                    "is_parallel": true,
                                                    "self": 129.9302173727483,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 128.82012952314108,
                                                            "count": 1127000,
                                                            "is_parallel": true,
                                                            "self": 128.82012952314108
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 3315.1875408074993,
                                                            "count": 1127000,
                                                            "is_parallel": true,
                                                            "self": 3315.1875408074993
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 601.2899556141347,
                                                            "count": 2254000,
                                                            "is_parallel": true,
                                                            "self": 307.02727071518893,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 294.26268489894574,
                                                                    "count": 9016000,
                                                                    "is_parallel": true,
                                                                    "self": 294.26268489894574
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 1419.538694586954,
                            "count": 2254000,
                            "self": 83.39974836914917,
                            "children": {
                                "process_trajectory": {
                                    "total": 292.62022181769134,
                                    "count": 2254000,
                                    "self": 290.67366681766,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 1.946555000031367,
                                            "count": 4,
                                            "self": 1.946555000031367
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 1043.5187244001136,
                                    "count": 126,
                                    "self": 425.1273845009855,
                                    "children": {
                                        "TorchPOCAOptimizer.update": {
                                            "total": 618.3913398991281,
                                            "count": 3801,
                                            "self": 618.3913398991281
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.4787912000028882,
                    "count": 1,
                    "self": 0.039519600017229095,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.4392715999856591,
                            "count": 2,
                            "self": 0.4392715999856591
                        }
                    }
                }
            }
        }
    }
}